---
title: "Homework 4"
author: "[Kaiwen Fu]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: html
# format: pdf
---

---

::: {.callout-important style="font-size: 0.8em;"}

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


We will be using the following libraries:


```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom"
)

# renv::install(packages)
sapply(packages, require, character.only=T)
```


<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 30 points
Automatic differentiation using `torch`
:::

###### 1.1 (5 points)

Consider $g(x, y)$ given by
$$
g(x, yy) = (x - 3)^2 + (y - 4)^2.
$$

Using elementary calculus derive the expressions for

$$
\frac{d}{dx}g(x, y), \quad \text{and} \quad \frac{d}{dy}g(x, y).
$$

Using your answer from above, what is the answer to
$$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} \quad \text{and} \quad \frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)} ?
$$

Define $g(x, y)$ as a function in R, compute the gradient of $g(x, y)$ with respect to $x=3$ and $y=4$. Does the answer match what you expected?

```{R}
# define
g <- function(x, y) {
  (x - 3)^2 + (y - 4)^2
}

#calculate
gradient_x <- 2 * (3 - 3)
gradient_y <- 2 * (4 - 4)

# print
gradient_x
gradient_y

#yes the answer matched what I exoected should be zero.
```


---

###### 1.2 (10 points)


$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$

Consider $h(\u, \v)$ given by
$$
h(\u, \v) = (\u \cdot \v)^3,
$$
where $\u \cdot \v$ denotes the dot product of two vectors, i.e., $\u \cdot \v = \sum_{i=1}^n u_i v_i.$

Using elementary calculus derive the expressions for the gradients

$$
\begin{aligned}
\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), \dots, \frac{d}{du_n}h(\u, \v)\Bigg)
\end{aligned}
$$

Using your answer from above, what is the answer to $\nabla_\u h(\u, \v)$ when $n=10$ and

$$
\begin{aligned}
\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
\end{aligned}
$$

Define $h(\u, \v)$ as a function in R, initialize the two vectors $\u$ and $\v$ as `torch_tensor`s. Compute the gradient of $h(\u, \v)$ with respect to $\u$. Does the answer match what you expected?

```{R}
library(torch)

# define
u <- torch_tensor(c(-1, 1, -1, 1, -1, 1, -1, 1, -1, 1), requires_grad=TRUE)
v <- torch_tensor(c(-1, -1, -1, -1, 1, 1, 1, 1, 1, 1))

# define function
h <- function(u, v) {
  torch_pow(torch_sum(u * v), 3)
}

# calculate
value_h <- h(u, v)


# calculate u
value_h$backward()

# output
u$grad


```



---

###### 1.3 (5 points)

Consider the following function
$$
f(z) = z^4 - 6z^2 - 3z + 4
$$

Derive the expression for 
$$
f'(z_0) = \frac{df}{dz}\Bigg|_{z=z_0}
$$
and evaluate $f'(z_0)$ when $z_0 = -3.5$.

Define $f(z)$ as a function in R, and using the `torch` library compute $f'(-3.5)$. 

```{R}
library(torch)

# define
f <- function(z) {
  z^4 - 6 * z^2 - 3 * z + 4
}

# define
f_prime <- function(z) {
  4 * z^3 - 12 * z - 3
}

# calculate at z0 = -3.5 
z0 <- torch_tensor(-3.5, requires_grad=TRUE)
value_f <- f(z0)
value_f$backward()

# output
z0$grad

```

---

###### 1.4 (5 points)

For the same function $f$, initialize $z[1] = -3.5$, and perform $n=100$ iterations of **gradient descent**, i.e., 

> $z[{k+1}] = z[k] - \eta f'(z[k]) \ \ \ \ $ for $k = 1, 2, \dots, 100$

Plot the curve $f$ and add taking $\eta = 0.02$, add the points $\{z_0, z_1, z_2, \dots z_{100}\}$ obtained using gradient descent to the plot. What do you observe?
```{R}
library(torch)

# define f(z)
f <- function(z) {
  z^4 - 6 * z^2 - 3 * z + 4
}

# define function f'(z)
f_prime <- function(z) {
  4 * z^3 - 12 * z - 3
}

# initailize
z <- list(torch_tensor(-3.5, requires_grad=TRUE))
eta <- 0.02

# do 100
for (k in 1:100) {

  value_f <- f(z[[k]])
  value_f$backward()

  
  z[[k + 1]] <- z[[k]] - eta * z[[k]]$grad
  
  z[[k + 1]] <- torch_tensor(as.numeric(z[[k + 1]]), requires_grad=TRUE)
}

# convert
z_values <- sapply(z, as.numeric)
f_values <- sapply(z_values, f)


plot_data <- data.frame(z = z_values, f = f_values)

# draw
ggplot(data = plot_data, aes(x = z, y = f)) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(x = z, y = f), color = "red")

```

---

###### 1.5 (5 points)


Redo the same analysis as **Question 1.4**, but this time using $\eta = 0.03$. What do you observe? What can you conclude from this analysis

```{R}
# new eta
eta <- 0.03


z <- list(torch_tensor(-3.5, requires_grad=TRUE))


for (k in 1:100) {
  value_f <- f(z[[k]])
  value_f$backward()
  z[[k + 1]] <- z[[k]] - eta * z[[k]]$grad
  z[[k + 1]] <- torch_tensor(as.numeric(z[[k + 1]]), requires_grad=TRUE)
}


z_values <- sapply(z, as.numeric)
f_values <- sapply(z_values, f)


plot_data <- data.frame(z = z_values, f = f_values)

ggplot(data = plot_data, aes(x = z, y = f)) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(x = z, y = f), color = "red")

# I found that as the number of iterations increases, the movement of the points stabilizes, indicating that we may have found a point near the minimum. This process shows how gradient descent gradually adjusts and approaches the goal in the process of finding the optimal solution.

```



<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 50 points
Logistic regression and interpretation of effect sizes
:::

For this question we will use the **Titanic** dataset from the Stanford data archive. This dataset contains information about passengers aboard the Titanic and whether or not they survived. 


---

###### 2.1 (5 points)

Read the data from the following URL as a tibble in R. Preprocess the data such that the variables are of the right data type, e.g., binary variables are encoded as factors, and convert all column names to lower case for consistency. Let's also rename the response variable `Survival` to `y` for convenience.


```{R}
library(tidyverse)

url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"


df <- read_csv(url) %>%
  rename_all(tolower) %>%
  rename(y = survived) %>%
  mutate(across(where(is.logical), as.factor))

# Output
head(df)

```

---

###### 2.2 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
library(corrplot)

numeric_data <- select_if(df, is.numeric)

# calculate
cor_matrix <- cor(numeric_data, use = "complete.obs") 

corrplot(cor_matrix, method = "circle")

```


---

###### 2.3 (10 points)

Fit a logistic regression model to predict the probability of surviving the titanic as a function of:

* `pclass`
* `sex`
* `age`
* `fare`
* `# siblings`
* `# parents`


```{R}
full_model <- glm(y ~ pclass + sex + age + fare + `siblings/spouses aboard` + `parents/children aboard`, data = df, family = "binomial")
summary(full_model)
```

---

###### 2.4 (30 points)

Provide an interpretation for the slope and intercept terms estimated in `full_model` in terms of the log-odds of survival in the titanic and in terms of the odds-ratio (if the covariate is also categorical).

::: {.callout-hint}
## 
Recall the definition of logistic regression from the lecture notes, and also recall how we interpreted the slope in the linear regression model (particularly when the covariate was categorical).
:::
```{R}
#In the Titanic data, the intercept of the logistic regression model tells us that without taking into account any other factors (such as cabin class, gender, etc.), the log odds of survival are positive, which means that the odds of survival are high To not survive. Each slope represents the impact of relevant factors on the probability of survival: the lower the cabin class, the lower the probability of survival; the probability of survival of men is lower than that of women; the probability of survival slightly decreases with each additional year of age; the higher the ticket price, the higher the probability of survival ;Passengers with siblings, spouses or parents and children on board have a slightly lower chance of survival.
```


<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 70 points

Variable selection and logistic regression in `torch`

:::


---

###### 3.1 (15 points)

Complete the following function `overview` which takes in two categorical vectors (`predicted` and `expected`) and outputs:

* The prediction accuracy
* The prediction error
* The false positive rate, and
* The false negative rate


```{R}
overview <- function(predicted, expected) {
  
  library(caret)
  predicted <- as.factor(predicted)
  expected <- as.factor(expected)
  cm <- confusionMatrix(predicted, expected)
  accuracy <- cm$overall['Accuracy']
  error <- 1 - accuracy
  false_positive_rate <- cm$byClass['False Positive Rate']
  false_negative_rate <- cm$byClass['False Negative Rate']
  
  return(data.frame(
    accuracy = accuracy,
    error = error,
    false_positive_rate = false_positive_rate,
    false_negative_rate = false_negative_rate
  ))
}

overview(df$y, df$y)


```



You can check if your function is doing what it's supposed to do by evaluating


and making sure that the accuracy is $100\%$ while the errors are $0\%$.
---

###### 3.2 (5 points)

Display an overview of the key performance metrics of `full_model`

```{R}
predicted_probabilities <- predict(full_model, newdata = df, type = "response")
predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)

predicted_factors <- as.factor(predicted_classes)

actual_factors <- as.factor(df$y)

performance_overview <- overview(predicted_factors, actual_factors)

print(performance_overview)

```


---

###### 3.3  (5 points)

Using backward-stepwise logistic regression, find a parsimonious altenative to `full_model`, and print its `overview`

```{R}
step_model <- step(full_model, direction = "backward")

```


---

###### 3.4  (15 points)

Using the `caret` package, setup a **$5$-fold cross-validation** training method using the `caret::trainConrol()` function

```{R}
library(caret)

controls <- trainControl(method = "cv", number = 5,
                         summaryFunction = twoClassSummary, 
                         classProbs = TRUE,
                         savePredictions = "final",
                         verboseIter = TRUE)


```

Now, using `control`, perform $5$-fold cross validation using `caret::train()` to select the optimal $\lambda$ parameter for LASSO with logistic regression. 


Take the search grid for $\lambda$ to be in $\{ 2^{-20}, 2^{-19.5}, 2^{-19}, \dots, 2^{-0.5}, 2^{0} \}$.

```{R}

lambda_grid <- 2^seq(-20, 0, by = 0.5)
tune_grid <- expand.grid(alpha = 1, lambda = lambda_grid)

df$sex_numeric <- ifelse(df$sex == "female", 1, 0)

predictor_columns <- c("pclass", "sex_numeric", "age", "siblings/spouses aboard", "parents/children aboard", "fare")

response_column <- "y"

lasso_fit <- train(x = df[, predictor_columns], 
                   y = df[[response_column]], 
                   method = "glmnet",
                   trControl = controls,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale"),
                   metric = "ROC",
                   family = "binomial")


print(lasso_fit)

plot(lasso_fit)


best_lambda <- lasso_fit$bestTune$lambda
cat("best lambda :", best_lambda, "\n")

```

Using the information stored in `lasso_fit$results`, plot the results for  cross-validation accuracy vs. $log_2(\lambda)$. Choose the optimal $\lambda^*$, and report your results for this value of $\lambda^*$.

---

###### 3.5  (25 points)

First, use the `model.matrix()` function to convert the covariates of `df` to a matrix format

```{R}
library(torch)

covariate_matrix <- model.matrix(~ . - 1, data = df)
```


Now, initialize the covariates $X$ and the response $y$ as `torch` tensors

```{R}
X <- torch_tensor(covariate_matrix, dtype = torch_float32())
y <- torch_tensor(as.numeric(df$y), dtype = torch_float32())
```


Using the `torch` library, initialize an `nn_module` which performs logistic regression for this dataset. (Remember that we have 6 different covariates)

```{R}
logistic <- nn_module(
  initialize = function() {
    self$fc <- nn_linear(dim(covariate_matrix)[2], 1)
  },
  forward = function(x) {
    output <- self$fc(x)
    torch_sigmoid(output)
  }
)

f <- logistic()
```


You can verify that your code is right by checking that the output to the following code is a vector of probabilities:

```{R}
f(X)
```



Now, define the loss function `Loss()` which takes in two tensors `X` and `y` and a function `Fun`, and outputs the **Binary cross Entropy loss** between `Fun(X)` and `y`. 

```{R}
Loss <- function(X, y, Fun) {
  predictions <- Fun(X)
  loss <- nnf_binary_cross_entropy(predictions, y)
  loss
}

```


Initialize an optimizer using `optim_adam()` and perform $n=1000$ steps of gradient descent in order to fit logistic regression using `torch`.

```{R}
f <- logistic()
optimizer <- optim_adam(f$parameters, lr = 0.01)

n <- 1000
for (i in 1:n) {
  optimizer$zero_grad()
  loss <- Loss(X, y, f)
  loss$backward()
  optimizer$step()
  if (i %% 100 == 0) {
    cat("Iteration:", i, "Loss:", loss$item(), "\n")
  }
}
```


Using the final, optimized parameters of `f`, compute the compute the predicted results on `X`

```{R}

predicted_probabilities <- f$forward(X)$detach()$sigmoid()$squeeze()

torch_predictions <- ifelse(predicted_probabilities > 0.5, 1, 0)

overview(torch_predictions, df$y)

```


---

###### 3.6  (5 points)

Create a summary table of the `overview()` summary statistics for each of the $4$ models we have looked at in this assignment, and comment on their relative strengths and drawbacks. 
```{R}
summary_df <- summary(df)

print(summary_df)
#This data summary table reflects the basic statistical information of Titanic passengers, showing the range and distribution of variables such as age, fares, etc., which helps us understand the characteristics of the data, but it does not directly reflect how these variables affect survival rates Wait for the prediction results of the specific model.
```


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::